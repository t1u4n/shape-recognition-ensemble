# -*- coding: utf-8 -*-
"""ensemble_shape_recognition.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/193g812D-pRDuJIH8FgpLQZ_JXu_DCDgE

<h1>Ensemble Methods for 2D Shape Recognition: Combining CNN and Transformer Models</h1>

<h2><b>Global Variables</b></h2>
"""

import os
from google.colab import drive
import shutil
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
from torchvision import datasets, models, transforms
drive.mount('/content/gdrive/')
os.chdir("/content/gdrive/My Drive/ECE549GroupProject")

# global variable
device = torch.device("cuda:0")
EPOCHS = 100
criterion = nn.CrossEntropyLoss()

from torchvision.datasets import ImageFolder
from torchvision.transforms import transforms
from torch.utils.data import DataLoader, random_split, ConcatDataset

# Preprocess
data_transform = transforms.Compose([
    # transforms.Resize(224),
    # transforms.CenterCrop(224),
    transforms.ToTensor(),
    # transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
])

# load data
data_dirs = ['data/a1/sin', 'data/a2/sin', 'data/a3/sin', 'data/a4/sin', 'data/a5/sin']
datasets = []
for data_dir in data_dirs:
    dataset = ImageFolder(data_dir, transform=data_transform)
    datasets.append(dataset)
dataset = ConcatDataset(datasets)

# Split dataset to train, val and test (8:1:!)
train_size = int(0.8 * len(dataset))
val_size = int(0.1 * len(dataset))
test_size = len(dataset) - train_size - val_size
train_set, val_set, test_set = random_split(dataset, [train_size, val_size, test_size])

# Define dataloader
train_dataloader = DataLoader(train_set, batch_size=32, shuffle=True, num_workers=2, pin_memory=True)
val_dataloader = DataLoader(val_set, batch_size=32, num_workers=2, pin_memory=True)
test_dataloader = DataLoader(test_set, batch_size=32, num_workers=2, pin_memory=True)

"""<h2><b>CNN: ResNet & MobileNet</b></h2>"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from torchvision.models import resnet50, resnet18, resnet34

class ResNet18(nn.Module):
    def __init__(self, num_classes):
        super(ResNet18, self).__init__()
        self.resnet18 = resnet18(pretrained=True)
        self.resnet18.fc = nn.Linear(512, num_classes)

    def forward(self, x):
        # print(f"ResNet18: {x.shape}")
        x = self.resnet18(x)
        # print(f"ResNet18: {x.shape}")
        x = x.type(torch.FloatTensor)
        # print(type(x))
        return x

class ResNet34(nn.Module):
    def __init__(self, num_classes):
        super(ResNet34, self).__init__()
        self.resnet34 = resnet34(pretrained=True)
        self.resnet34.fc = nn.Linear(512, num_classes)

    def forward(self, x):
        # print(f"ResNet34: {x.shape}")
        x = self.resnet34(x)
        # print(f"ResNet34: {x.shape}")
        x = x.type(torch.FloatTensor)
        # print(type(x))
        return x

class ResNet50(nn.Module):
    def __init__(self, num_classes):
        super(ResNet50, self).__init__()
        self.resnet50 = resnet50(pretrained=True)
        self.resnet50.fc = nn.Linear(2048, num_classes)

    def forward(self, x):
        # print(f"ResNet50: {x.shape}")
        x = self.resnet50(x)
        # print(f"ResNet50: {x.shape}")
        x = x.type(torch.FloatTensor)
        # print(type(x))
        return x
  
class VGG11(nn.Module):
    def __init__(self, num_classes):
        super(VGG11, self).__init__()
        self.vgg11 = models.vgg11(pretrained=True)
        self.vgg11.classifier[6] = nn.Linear(4096, num_classes)

    def forward(self, x):
        x = self.vgg11(x)
        x = x.type(torch.FloatTensor)
        return x

class VGG16(nn.Module):
    def __init__(self, num_classes):
        super(VGG16, self).__init__()
        self.vgg16 = models.vgg16(pretrained=True)
        self.vgg16.classifier[6] = nn.Linear(4096, num_classes)

    def forward(self, x):
        x = self.vgg16(x)
        x = x.type(torch.FloatTensor)
        return x

# Train Method
import matplotlib.pyplot as plt
import time

def train_model(model, num_epochs, train_dataloader, val_dataloader, device, learning_rate=0.0001):
    model = model.to(device)
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.SGD(model.parameters(),
                          lr=learning_rate,
                          momentum=0.9,
                          weight_decay=0.0001)
    train_size = len(train_dataloader.dataset)
    val_size = len(val_dataloader.dataset)

    train_loss_history = []
    train_acc_history = []
    val_loss_history = []
    val_acc_history = []
    start_time = time.time()
    for epoch in range(num_epochs):
        # training phase
        model.train()
        running_loss = 0.0
        running_corrects = 0
        for inputs, labels in train_dataloader:
            inputs = inputs.to(device)
            labels = labels.to(device)
            optimizer.zero_grad()
            outputs = model(inputs).to(device)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()
            running_loss += loss.item() * inputs.size(0)
            _, preds = torch.max(outputs, 1)
            running_corrects += torch.sum(preds == labels.data)
        epoch_loss = running_loss / train_size
        epoch_acc = running_corrects / train_size * 100.0
        train_loss_history.append(epoch_loss)
        train_acc_history.append(epoch_acc.cpu())
        print('[Train #{}] Loss: {:.4f} Acc: {:.4f}% Time: {:.4f}s'.format(epoch, epoch_loss, epoch_acc, time.time() - start_time))

        # testing phase
        model.eval()
        with torch.no_grad():
            running_loss = 0.0
            running_corrects = 0
            for inputs, labels in val_dataloader:
                inputs = inputs.to(device)
                labels = labels.to(device)
                outputs = model(inputs).to(device)
                loss = criterion(outputs, labels)
                running_loss += loss.item() * inputs.size(0)
                _, preds = torch.max(outputs, 1)
                running_corrects += torch.sum(preds == labels.data)
            epoch_loss = running_loss / val_size
            epoch_acc = running_corrects / val_size * 100.0
            val_loss_history.append(epoch_loss)
            val_acc_history.append(epoch_acc.cpu())
            print('[Val #{}] Loss: {:.4f} Acc: {:.4f}% Time: {:.4f}s'.format(epoch, epoch_loss, epoch_acc, time.time() - start_time))

    # plot loss and accuracy curves
    fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(10, 4))
    ax[0].plot(train_loss_history, label='train loss')
    ax[0].plot(val_loss_history, label='val loss')
    ax[0].set_xlabel('epoch')
    ax[0].set_ylabel('loss')
    ax[0].legend()
    ax[1].plot(train_acc_history, label='train acc')
    ax[1].plot(val_acc_history, label='val acc')
    ax[1].set_xlabel('epoch')
    ax[1].set_ylabel('accuracy')
    ax[1].legend()
    plt.show()

# vgg_model = VGG11(num_classes=20)
vgg_model = VGG16(num_classes=20)
train_model(vgg_model, 20, train_dataloader, val_dataloader, device)

# Pretrained ResNet18
resnet50_model = ResNet50(num_classes=20)
train_model(resnet50_model, 20, train_dataloader, val_dataloader, device)

class MobileNet(nn.Module):
    def __init__(self, num_classes):
        super(MobileNet, self).__init__()
        self.mobilenet = models.mobilenet_v2(pretrained=True)
        self.mobilenet.classifier[1] = nn.Linear(1280, num_classes)

    def forward(self, x):
        x = self.mobilenet(x)
        return x



# Mobile Model
mobilenet_model = MobileNet(num_classes=20)
train_model(mobilenet_model, 15, train_dataloader, val_dataloader, device)

class TransformerModel(nn.Module):
    def __init__(self, input_dim, output_dim, hidden_dim, num_layers, num_heads):
        super(TransformerModel, self).__init__()

        self.input_dim = input_dim
        self.output_dim = output_dim
        self.hidden_dim = hidden_dim
        self.num_layers = num_layers
        self.num_heads = num_heads

        self.input_projection = nn.Linear(input_dim, hidden_dim)
        assert hidden_dim % num_heads == 0, "hidden_dim must be divisible by num_heads"

        self.encoder_layer = nn.TransformerEncoderLayer(d_model=hidden_dim, nhead=num_heads, dim_feedforward=hidden_dim, dropout=0.1)
        self.encoder = nn.TransformerEncoder(self.encoder_layer, num_layers=num_layers)
        self.decoder_layer = nn.TransformerDecoderLayer(d_model=output_dim, nhead=num_heads, dim_feedforward=hidden_dim, dropout=0.1)
        self.decoder = nn.TransformerDecoder(self.decoder_layer, num_layers=num_layers)

        self.output_projection = nn.Linear(hidden_dim, output_dim)

    def forward(self, src, trg):
        src = src.permute(0, 2, 3, 1)  # [batch_size, img_height, img_width, channel_num]
        src = self.input_projection(src)
        src = src.flatten(start_dim=1, end_dim=-2)  # [batch_size, seq_len, hidden_dim]

        trg = trg.permute(0, 2, 3, 1)  # [batch_size, img_height, img_width, channel_num]
        trg = trg[:, :-1, :] 
        trg = self.output_projection(trg)  # Map to output_dim
        trg = trg.permute(1, 0, 2)  # [seq_len, batch_size, output_dim]

        memory = self.encoder(src)
        output = self.decoder(trg, memory)

        output = output.permute(1, 0, 2)  # [batch_size, seq_len, hidden_dim]
        output = self.output_projection(output)  # Map to output_dim
        return output

class Transformer(nn.Module):
    def __init__(self, input_dim, hidden_dim, output_dim, num_layers, num_heads, dropout):
        super().__init__()

        self.num_patches = 14 * 14  # number of patches to be extracted from the input image
        self.patch_size = 16  # size of each patch
        self.proj = nn.Linear(input_dim * self.patch_size ** 2, hidden_dim)  # project the patches to a sequence of tokens
        self.transformer = nn.TransformerEncoder(
            nn.TransformerEncoderLayer(d_model=hidden_dim, nhead=num_heads, dim_feedforward=hidden_dim * 4, dropout=dropout, activation='relu'),
            num_layers=num_layers
        )
        self.fc = nn.Linear(hidden_dim, output_dim)
        self.softmax = nn.Softmax(dim=1)

    def forward(self, x):
        # x: [batch_size, input_dim, image_height, image_width]
        batch_size, input_dim, image_height, image_width = x.shape
        # print(x.shape)
        x = F.unfold(x, self.patch_size, stride=self.patch_size)  # [batch_size, input_dim * patch_size ** 2, num_patches]
        # print(x.shape)
        x = x.transpose(1, 2)  # [batch_size, num_patches, input_dim * patch_size ** 2]
        # print(x.shape)
        x = self.proj(x)  # [batch_size, num_patches, hidden_dim]
        # print(x.shape)
        x = x.transpose(0, 1)  # [num_patches, batch_size, hidden_dim]
        # print(x.shape)
        x = self.transformer(x)  # [num_patches, batch_size, hidden_dim]
        # print(x.shape)
        x = x.mean(dim=0)  # [batch_size, hidden_dim]
        x = self.fc(x)  # [batch_size, output_dim]
        # print(x.shape)
        x = self.softmax(x)  # [batch_size, output_dim]
        # print(x.shape)
        x = x.type(torch.FloatTensor)
        return x

transformer_model = Transformer(input_dim=3, output_dim=20, hidden_dim=512, num_layers=6, num_heads=4, dropout=0.1).to(device)
train_model(transformer_model, 250, train_dataloader, val_dataloader, device, learning_rate=0.005)

from torchvision.transforms import Resize

class ViTModel(nn.Module):
    def __init__(self, num_classes):
        super().__init__()
        self.transform = Resize((224, 224))
        self.vit_model = models.vit_b_16(pretrained=True)
        self.output = nn.Linear(1000, num_classes)
        # self.vit_model.fc = nn.Linear(self.vit_model.fc.in_features, num_classes)

    def forward(self, x):
        # print(x.shape)
        x = self.transform(x)
        x = self.vit_model(x)
        x = self.output(x)
        # print(x.shape)
        return x

vit_model = ViTModel(num_classes=20)
train_model(vit_model, 10, train_dataloader, val_dataloader, device, learning_rate=0.0001)

train_loss_history = [2.5991,1.6971,1.1711, 0.8463, 0.6386, 0.4922, 0.3761, 0.2709, 0.1903, 0.1388, 0.0924]
val_loss_history = [2.0139,1.4239, 1.0339, 0.7836, 0.6507, 0.6433, 0.4581, 0.3914, 0.3690, 0.3212, 0.2923]
train_acc_history = [22.125,49.6375, 64.7000, 74.7500, 81.3875, 85.8625, 89.6750, 93.3125, 95.9500, 97.6250, 98.9625]
val_acc_history = [41.3, 56.6000, 68.1000, 75.7000, 80.6000, 79.4000, 86.4000, 88.7000, 88.6000, 89.7000, 90.8000]

fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(10, 4))
ax[0].plot(train_loss_history, label='train loss')
ax[0].plot(val_loss_history, label='val loss')
ax[0].set_xlabel('epoch')
ax[0].set_ylabel('loss')
ax[0].legend()
ax[1].plot(train_acc_history, label='train acc')
ax[1].plot(val_acc_history, label='val acc')
ax[1].set_xlabel('epoch')
ax[1].set_ylabel('accuracy')
ax[1].legend()
plt.show()

class SwinTransformerModel(nn.Module):
    def __init__(self, num_classes):
        super().__init__()
        self.transform = Resize((224, 224))
        self.vit_model = models.swin_t(pretrained=True)
        self.output = nn.Linear(1000, num_classes)
        # self.vit_model.fc = nn.Linear(self.vit_model.fc.in_features, num_classes)

    def forward(self, x):
        # print(x.shape)
        x = self.transform(x)
        x = self.vit_model(x)
        x = self.output(x)
        # print(x.shape)
        return x

swin_model = SwinTransformerModel(20)
train_model(swin_model, 20, train_dataloader, val_dataloader, device, learning_rate=0.0001)

#######################################
#######  Ensemble Model   #######
#######################################

import torch
import torch.nn as nn
from typing import List, Callable

class Ensemble(nn.Module):
    def __init__(self, models: List[nn.Module], classifier: Callable[[torch.Tensor], torch.Tensor]):
        super(Ensemble, self).__init__()
        self.models = nn.ModuleList(models)
        self.classifier = classifier

    def forward(self, x):
        # print(x.shape)
        outputs = []
        for model in self.models:
            outputs.append(model(x).to(device))
        # print(outputs)
        combined_outputs = torch.stack(outputs, dim=0)
        combined_outputs = self.classifier(combined_outputs)
        return combined_outputs

#######################################
#####  Classifier methods   #####
#######################################

# Voting Method
# We do not use this model because we only have two models
def voting_classifier(outputs):
    # outputs: a list or a tensor containing the predicted results from multiple models
    # Compute the predicted classes for each model
    predicted = torch.argmax(outputs, dim=2)  
    predicted = predicted.permute(1, 0)  
    mode, _ = torch.mode(predicted, dim=1)  
    # print(mode)
    return mode


# Average Method
def average_classifier(outputs):
    return torch.mean(outputs, dim=0)

# Avg
# print(model_list)
model_list = [vgg_model, resnet50_model, mobilenet_model, vit_model, swin_model]
model = Ensemble(model_list, average_classifier)
model.eval()
test_loss_history = []
test_acc_history = []
with torch.no_grad():
  running_loss = 0.0
  running_corrects = 0
  for inputs, labels in test_dataloader:
      inputs = inputs.to(device)
      labels = labels.to(device)
      outputs = model(inputs).to(device)
      loss = criterion(outputs, labels)
      running_loss += loss.item() * inputs.size(0)
      _, preds = torch.max(outputs, 1)
      running_corrects += torch.sum(preds == labels.data)
  epoch_loss = running_loss / test_size
  epoch_acc = running_corrects / test_size * 100.0
  test_loss_history.append(epoch_loss)
  test_acc_history.append(epoch_acc.cpu())
  print('Loss: {:.4f} Acc: {:.4f}%'.format(epoch_loss, epoch_acc))

# Voting
# print(model_list)
model_list = [vgg_model, resnet50_model, mobilenet_model, vit_model, swin_model]
model = Ensemble(model_list, voting_classifier)
model.eval()
test_loss_history = []
test_acc_history = []
with torch.no_grad():
  running_loss = 0.0
  running_corrects = 0
  for inputs, labels in test_dataloader:
      inputs = inputs.to(device)
      labels = labels.to(device)
      preds = model(inputs).to(device)
      running_corrects += torch.sum(preds == labels.data)
  epoch_loss = running_loss / test_size
  epoch_acc = running_corrects / test_size * 100.0
  test_loss_history.append(epoch_loss)
  test_acc_history.append(epoch_acc.cpu())
  print('Loss: {:.4f} Acc: {:.4f}%'.format(epoch_loss, epoch_acc))

# Fusion: train a metamodel to predict
# Train Method
import matplotlib.pyplot as plt
import time

def train_meta_model(meta_model, model_list, num_epochs, train_dataloader, val_dataloader, device, learning_rate=0.0001):
    meta_model = meta_model.to(device)
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.SGD(meta_model.parameters(),
                          lr=learning_rate,
                          momentum=0.9,
                          weight_decay=0.0001)
    train_size = len(train_dataloader.dataset)
    val_size = len(val_dataloader.dataset)

    train_loss_history = []
    train_acc_history = []
    val_loss_history = []
    val_acc_history = []
    start_time = time.time()
    for epoch in range(num_epochs):
        # training phase
        meta_model.train()
        running_loss = 0.0
        running_corrects = 0
        for inputs, labels in train_dataloader:
            inputs = inputs.to(device)
            model_outputs = []
            for model in model_list:
                model_output = model(inputs).to(device)
                model_outputs.append(model_output)
            model_outputs = torch.stack(model_outputs, dim=0)
            labels = labels.to(device)
            optimizer.zero_grad()
            meta_output = meta_model(model_outputs).to(device)
            loss = criterion(meta_output, labels)
            loss.backward()
            optimizer.step()
            running_loss += loss.item() * inputs.size(0)
            _, preds = torch.max(meta_output, 1)
            running_corrects += torch.sum(preds == labels.data)
        epoch_loss = running_loss / train_size
        epoch_acc = running_corrects / train_size * 100.0
        train_loss_history.append(epoch_loss)
        train_acc_history.append(epoch_acc.cpu())
        print('[Train #{}] Loss: {:.4f} Acc: {:.4f}% Time: {:.4f}s'.format(epoch, epoch_loss, epoch_acc, time.time() - start_time))

        # testing phase
        meta_model.eval()
        with torch.no_grad():
            running_loss = 0.0
            running_corrects = 0
            for inputs, labels in val_dataloader:
                inputs = inputs.to(device)
                labels = labels.to(device)
                model_outputs = []
                for model in model_list:
                    model_output = model(inputs).to(device)
                    model_outputs.append(model_output)
                model_outputs = torch.stack(model_outputs, dim=0)
                meta_output = meta_model(model_outputs).to(device)
                loss = criterion(meta_output, labels)
                running_loss += loss.item() * inputs.size(0)
                _, preds = torch.max(meta_output, 1)
                running_corrects += torch.sum(preds == labels.data)
            epoch_loss = running_loss / val_size
            epoch_acc = running_corrects / val_size * 100.0
            val_loss_history.append(epoch_loss)
            val_acc_history.append(epoch_acc.cpu())
            print('[Val #{}] Loss: {:.4f} Acc: {:.4f}% Time: {:.4f}s'.format(epoch, epoch_loss, epoch_acc, time.time() - start_time))

    # plot loss and accuracy curves
    fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(10, 4))
    ax[0].plot(train_loss_history, label='train loss')
    ax[0].plot(val_loss_history, label='val loss')
    ax[0].set_xlabel('epoch')
    ax[0].set_ylabel('loss')
    ax[0].legend()
    ax[1].plot(train_acc_history, label='train acc')
    ax[1].plot(val_acc_history, label='val acc')
    ax[1].set_xlabel('epoch')
    ax[1].set_ylabel('accuracy')
    ax[1].legend()
    plt.show()

class MetaClassifier(nn.Module):
    def __init__(self, num_models, num_classes):
        super(MetaClassifier, self).__init__()
        self.num_classes = num_classes
        self.num_models = num_models
        self.fc1 = nn.Linear(num_models*num_classes, 512)
        self.fc2 = nn.Linear(512, 256)
        self.fc3 = nn.Linear(256, num_classes)
        self.dropout = nn.Dropout(0.5)

    def forward(self, x):
        x = x.view(-1, self.num_models*self.num_classes)
        x = F.relu(self.fc1(x))
        x = self.dropout(x)
        x = F.relu(self.fc2(x))
        x = self.dropout(x)
        x = self.fc3(x)
        return x



# Define the meta-classifier and optimizer
meta_model = MetaClassifier(4, 20).to(device)
model_list = [vgg_model, resnet50_model, mobilenet_model, vit_model]

train_meta_model(meta_model, model_list, 50, train_dataloader, val_dataloader, device, learning_rate=0.001)